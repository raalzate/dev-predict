{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del Código de Entrenamiento de Modelo LSTM para Estimación de Historias de Usuario\n",
    "\n",
    "Este notebook analiza el código proporcionado para entrenar un modelo LSTM multi-tarea que estima esfuerzo, tiempo y complejidad en historias de usuario basadas en texto (título y Gherkin) y características numéricas extraídas. Como experto en Machine Learning, desglosaré el código, explicaré su funcionalidad, identificaré posibles mejoras y problemas, y proporcionaré ejemplos con un dataset simulado.\n",
    "\n",
    "**Notas generales sobre el código:**\n",
    "- Usa PyTorch para el modelo LSTM.\n",
    "- Incluye preprocesamiento de texto simple (tokenización, vocabulario, padding).\n",
    "- Ingeniería de características basada en keywords en Gherkin.\n",
    "- Multi-tarea: regresión para esfuerzo y tiempo, clasificación para complejidad.\n",
    "- Posibles mejoras: usar embeddings pre-entrenados, validación cruzada, manejo de desbalanceo, etc.\n",
    "\n",
    "**Dependencias:** Asegúrate de tener instaladas las siguientes librerías:\n",
    "```bash\n",
    "pip install pandas torch joblib numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from collections import Counter\n",
    "import joblib\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, precision_recall_fscore_support\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "import spacy\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase TrainingConfig\n",
    "\n",
    "Define constantes para el entrenamiento, como rutas, dimensiones del modelo y parámetros de entrenamiento. Es una buena práctica para mantener la configuración centralizada.\n",
    "\n",
    "**Análisis:**\n",
    "- **Ventajas:** Facilita la reproducibilidad.\n",
    "- **Mejoras:** Podría usar un archivo YAML para configs más complejas, o dataclass para tipado estricto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    CSV_PATH = '../stories_dataset.csv'\n",
    "    MODEL_PATH = 'lstm_model.pth'\n",
    "    PROCESSOR_PATH = 'text_processor.pkl'\n",
    "    SCALER_PATH = 'scaler.pkl'\n",
    "    \n",
    "    EMBEDDING_DIM = 512\n",
    "    HIDDEN_DIM = 512  \n",
    "    BIDIRECTIONAL = True\n",
    "    DROPOUT = 0.5\n",
    "    NUM_LAYERS = 3\n",
    "    \n",
    "    NUM_EPOCHS = 150\n",
    "    LEARNING_RATE = 0.00005\n",
    "    BATCH_SIZE = 16\n",
    "    PATIENCE = 15\n",
    "    LOSS_WEIGHTS = {'effort': 1.0, 'time': 2.0}\n",
    "    \n",
    "    MAX_SEQ_LEN = 500\n",
    "    MIN_WORD_FREQ = 1\n",
    "    \n",
    "    NUMERICAL_FEATURES = [\n",
    "        'gherkin_steps', 'gherkin_length', 'num_scenarios', 'num_technical_terms',\n",
    "        'num_conditions', 'num_entities', 'num_roles',\n",
    "        'has_frontend', 'has_backend', 'has_security', 'has_payment', 'has_crud',\n",
    "        'has_reporting', 'has_integration', 'has_notification', 'has_devops_mlops',\n",
    "        'has_accessibility', 'has_mobile', 'has_testing', 'has_error_handling',\n",
    "        'has_ui_interaction', 'has_database_query', 'tech_java', 'tech_node',\n",
    "        'tech_python', 'tech_frontend_framework', 'tech_database', 'tech_infra_cloud'\n",
    "    ]\n",
    "    TARGET_COLUMNS = ['effort', 'time']\n",
    "    TEXT_COLUMNS = ['title', 'gherkin']\n",
    "    USE_SPACY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation LSTM\n",
    "\n",
    "Modelo LSTM multi-tarea que toma embeddings de texto concatenados con features numéricas y predice tres salidas.\n",
    "\n",
    "**Análisis:**\n",
    "- **Entrada:** Texto tokenizado (secuencia) + features numéricas (repetidas por timestep).\n",
    "- **Salidas:** Regresión lineal para effort y time; softmax implícito para complejidad (3 clases).\n",
    "- **Fortalezas:** Multi-tarea eficiente, usa LSTM para capturar secuencias en texto.\n",
    "- **Debilidades:** Embedding desde cero (no pre-entrenado como GloVe/BERT). LSTM simple (no bidireccional). No dropout para regularización.\n",
    "- **Mejoras:** Agregar dropout, usar GRU en lugar de LSTM, o transformer para mejor manejo de secuencias largas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimationLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_features: int, bidirectional: bool, dropout: float, num_layers: int, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        lstm_dim = embedding_dim\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(lstm_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_effort = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * num_directions + num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.fc_time = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * num_directions + num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, text: torch.Tensor, numerical_features: torch.Tensor):\n",
    "        embedded = self.embedding(text)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        last_hidden = out[:, -1, :]\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "        combined = torch.cat((last_hidden, numerical_features), dim=1)\n",
    "        effort = self.fc_effort(combined)\n",
    "        time_est = self.fc_time(combined)\n",
    "        return effort, time_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processor\n",
    "\n",
    "Maneja tokenización, construcción de vocabulario y conversión a secuencias.\n",
    "\n",
    "**Análisis:**\n",
    "- Tokenización simple con regex (solo palabras alfanuméricas).\n",
    "- Vocab con padding y unk.\n",
    "- **Fortalezas:** Ligero y personalizado.\n",
    "- **Debilidades:** No maneja stemming/lemmatization, ni subwords. MIN_WORD_FREQ=1 incluye todo (posible ruido).\n",
    "- **Mejoras:** Usar NLTK/Spacy para tokenización avanzada, o tokenizer de HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, max_seq_len: int, min_freq: int, use_spacy: bool = False):\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.min_freq = min_freq\n",
    "        self.use_spacy = use_spacy\n",
    "        self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        if self.use_spacy:\n",
    "            doc = self.nlp(text.lower())\n",
    "            return [token.text for token in doc]\n",
    "        return re.findall(r'\\b\\w+\\.\\w+\\b|\\b\\w+\\b|[.,!?;]', text.lower())\n",
    "\n",
    "    def build_vocab(self, texts: pd.Series):\n",
    "        all_words = []\n",
    "        for text in texts:\n",
    "            all_words.extend(self._tokenize(text))\n",
    "        word_counts = Counter(all_words)\n",
    "        for word, freq in word_counts.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.vocab[word] = len(self.vocab)\n",
    "        logging.info(f\"Vocabulario construido con {len(self.vocab)} tokens.\")\n",
    "      \n",
    "\n",
    "    def text_to_sequence(self, text: str) -> List[int]:\n",
    "        words = self._tokenize(text)\n",
    "        seq = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
    "        if len(seq) > self.max_seq_len:\n",
    "            seq = seq[:self.max_seq_len]\n",
    "        else:\n",
    "            seq += [self.vocab['<PAD>']] * (self.max_seq_len - len(seq))\n",
    "        return seq\n",
    "\n",
    "    def save(self, path: str):\n",
    "        joblib.dump(self, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str):\n",
    "        return joblib.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Dataset\n",
    "\n",
    "Dataset para PyTorch, combina texto procesado con features numéricas y targets.\n",
    "\n",
    "**Análisis:**\n",
    "- Maneja conversión a tensores.\n",
    "- Incluye fix para NaNs (buena práctica).\n",
    "- **Debilidades:** Asume 'full_text' existe; complejidad mapeada hardcoded.\n",
    "- **Mejoras:** Agregar data augmentation para texto (e.g., synonyms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, text_processor: TextProcessor, numerical_cols: List[str], scaler: StandardScaler = None):\n",
    "        self.df = df\n",
    "        self.text_processor = text_processor\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.scaler = scaler\n",
    "        if self.scaler:\n",
    "            self.df[numerical_cols] = self.scaler.transform(self.df[numerical_cols])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        item = self.df.iloc[idx]\n",
    "        full_text = item['full_text']\n",
    "        seq_tensor = torch.LongTensor(self.text_processor.text_to_sequence(full_text))\n",
    "        \n",
    "        numerical_values = pd.to_numeric(item[self.numerical_cols], errors='coerce').fillna(0).values\n",
    "        numerical_features = torch.tensor(numerical_values, dtype=torch.float32)\n",
    "        \n",
    "        effort = torch.tensor(float(item['effort']), dtype=torch.float32)\n",
    "        time_est = torch.tensor(float(item['time']), dtype=torch.float32)\n",
    "        \n",
    "        return seq_tensor, numerical_features, effort, time_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función extract_features\n",
    "\n",
    "Ingeniería de características: cuenta steps en Gherkin, flags basados en keywords.\n",
    "\n",
    "**Análisis:**\n",
    "- Simple pero efectiva para domain-specific features.\n",
    "- Usa regex contains (case-insensitive).\n",
    "- Crea 'complexity' basada en effort (umbral hardcoded).\n",
    "- **Mejoras:** Usar NLP más avanzado (e.g., TF-IDF, BERT para features), o aprender features end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica una ingeniería de características robusta, manejando la ausencia\n",
    "    opcional de la columna 'unit_tests' y detectando tecnologías.\n",
    "    \"\"\"\n",
    "    logging.info(\"Aplicando ingeniería de características robusta...\")\n",
    "    df_featured = df.copy()\n",
    "\n",
    "    # --- 1. Validación de Columnas Esenciales y Limpieza ---\n",
    "    expected_cols = ['id', 'title', 'gherkin', 'effort', 'time']\n",
    "    if not all(col in df_featured.columns for col in expected_cols):\n",
    "        missing = [col for col in expected_cols if col not in df_featured.columns]\n",
    "        raise ValueError(f\"Columnas esenciales faltantes en el dataset: {missing}\")\n",
    "\n",
    "    df_featured['gherkin'] = df_featured['gherkin'].astype(str).fillna('')\n",
    "    df_featured['title'] = df_featured['title'].astype(str).fillna('')\n",
    "\n",
    "    # --- 2. Creación del Texto Base para Análisis ---\n",
    "    text_for_keywords = df_featured['title'] + \" \" + df_featured['gherkin']\n",
    "\n",
    "    # --- 3. Características Basadas en Categorías y Tecnologías ---\n",
    "    keyword_categories = {\n",
    "        'has_frontend': r'frontend|UI|interfaz|CSS|React|Angular|Vue|diseño|vista|pantalla',\n",
    "        'has_backend': r'backend|servidor|database|base de datos|bd|API|endpoint|SQL|servicios|microservicio|Java|NodeJS|NestJS|Python',\n",
    "        'has_security': r'seguridad|security|JWT|OAuth|token|autenticación|contraseña|encriptar|CSRF|XSS',\n",
    "        'has_payment': r'pago|payment|stripe|paypal|tarjeta de crédito|checkout|factura|compra',\n",
    "        'has_crud': r'crear|añadir|guardar|editar|actualizar|modificar|eliminar|borrar|ver|listar|obtener',\n",
    "        'has_reporting': r'reporte|dashboard|gráfico|exportar|CSV|PDF|Excel|analíticas|métricas',\n",
    "        'has_integration': r'api externa|third-party|integración|webhook|sincronizar|CRM|ERP',\n",
    "        'has_notification': r'notificación|email|correo|SMS|push|alerta|mensaje',\n",
    "        'has_devops_mlops': r'CI/CD|pipeline|deploy|despliegue|Kubernetes|Docker|monitor|observabilidad|modelo|ML|IA|DevOps',\n",
    "        'has_accessibility': r'accesibilidad|accessibility|WCAG|lector de pantalla|screen reader|ARIA',\n",
    "        'has_mobile': r'móvil|app|push|biometría|offline|geolocalización|cámara|gesto',\n",
    "        'has_testing': r'test|prueba|mock|verificar|validar|assertion|simula',\n",
    "        'has_error_handling': r'error|excepción|exception|fallo|failure|validar|manejo de error',\n",
    "        'has_ui_interaction': r'clic|seleccionar|navegar|click|select',\n",
    "        'has_database_query': r'query|select|sql|database|base de datos|bd'\n",
    "    }\n",
    "    for feature_name, pattern in keyword_categories.items():\n",
    "        df_featured[feature_name] = text_for_keywords.str.contains(pattern, case=False, regex=True, na=False).astype(int)\n",
    "\n",
    "    tech_stack_keywords = {\n",
    "        'tech_java': r'java|spring|maven|gradle|JPA|hibernate',\n",
    "        'tech_node': r'node\\.?js|nestjs|express|npm|yarn',\n",
    "        'tech_python': r'python|django|flask|fastapi|pip',\n",
    "        'tech_frontend_framework': r'react|angular|vue|svelte',\n",
    "        'tech_database': r'sql|mysql|postgres|mongodb|redis|base de datos|database',\n",
    "        'tech_infra_cloud': r'aws|azure|gcp|docker|kubernetes|terraform|S3'\n",
    "    }\n",
    "    for tech_name, pattern in tech_stack_keywords.items():\n",
    "        df_featured[tech_name] = text_for_keywords.str.contains(pattern, case=False, regex=True, na=False).astype(int)\n",
    "\n",
    "    # --- 4. Características Cuantitativas y Estructurales ---\n",
    "    gherkin_keywords = [r'\\bGiven\\b', r'\\bWhen\\b', r'\\bThen\\b', r'\\bAnd\\b', r'\\bDado\\b', r'\\bCuando\\b', r'\\bEntonces\\b', r'\\bY\\b']\n",
    "    df_featured['gherkin_steps'] = df_featured['gherkin'].apply(lambda x: sum(len(re.findall(word, x, re.IGNORECASE)) for word in gherkin_keywords))\n",
    "    df_featured['gherkin_length'] = df_featured['gherkin'].str.len()\n",
    "    df_featured['num_scenarios'] = df_featured['gherkin'].str.count(r'Scenario:|Escenario:', re.IGNORECASE)\n",
    "    entities = ['usuario', 'cliente', 'administrador', 'admin', 'sistema', 'desarrollador', 'visitante']\n",
    "    df_featured['num_entities'] = df_featured['gherkin'].apply(lambda x: sum(x.lower().count(entity) for entity in entities))\n",
    "    df_featured['num_roles'] = df_featured['gherkin'].str.lower().str.count('admin|user|cliente')\n",
    "    technical_terms = ['api', 'database', 'authentication', 'frontend', 'backend']\n",
    "    df_featured['num_technical_terms'] = df_featured['gherkin'].apply(lambda x: sum(x.lower().count(term) for term in technical_terms))\n",
    "    df_featured['num_conditions'] = df_featured['gherkin'].str.lower().str.count('if|when|si')\n",
    "    \n",
    "    # --- 5. Creación del Texto Combinado y Limpieza Final ---\n",
    "    df_featured['full_text'] = df_featured['title'] + \" \" + df_featured['gherkin']\n",
    "    \n",
    "    if (df_featured['effort'] < 0).any() or (df_featured['time'] < 0).any():\n",
    "        raise ValueError(\"Valores negativos encontrados en 'effort' o 'time'\")\n",
    "        \n",
    "    numerical_cols = TrainingConfig.NUMERICAL_FEATURES\n",
    "    for col in numerical_cols:\n",
    "        df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce').fillna(0).astype(float)\n",
    "    \n",
    "    logging.info(\"Ingeniería de características robusta finalizada con éxito.\")\n",
    "    return df_featured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función run_training_pipeline\n",
    "\n",
    "Orquesta todo: carga data, procesa, entrena, guarda. Modificada para usar df de ejemplo y visualizar pérdidas.\n",
    "\n",
    "**Análisis:**\n",
    "- Buen flujo end-to-end.\n",
    "- Usa MSE para regresión, CE para clasificación.\n",
    "- No hay validación (overfitting posible). Imprime loss cada 5 epochs.\n",
    "- **Mejoras:** Agregar set de validación, early stopping, métricas (MAE para regresión, accuracy para clasificación). Usar wandb para logging.\n",
    "\n",
    "Nota: Usamos batch_size=2 y 5 epochs para el ejemplo pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device, criterion_regression, weights):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    effort_loss, time_loss = 0, 0\n",
    "    all_effort_preds, all_effort_true = [], []\n",
    "    all_time_preds, all_time_true = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq, numerical, effort, time_est in loader:\n",
    "            seq, numerical, effort, time_est = (\n",
    "                seq.to(device), numerical.to(device), effort.to(device),\n",
    "                time_est.to(device)\n",
    "            )\n",
    "            effort_pred, time_pred = model(seq, numerical)\n",
    "            \n",
    "            # Usar view(-1) para asegurar formas compatibles\n",
    "            loss_effort = criterion_regression(effort_pred.view(-1), effort)\n",
    "            loss_time = criterion_regression(time_pred.view(-1), time_est)\n",
    "            loss = (weights['effort'] * loss_effort) + (weights['time'] * loss_time)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            effort_loss += loss_effort.item()\n",
    "            time_loss += loss_time.item()\n",
    "            \n",
    "            # Convertir a 1D para evitar problemas con batch size = 1\n",
    "            effort_pred_1d = torch.atleast_1d(effort_pred.view(-1)).cpu().numpy()\n",
    "            time_pred_1d = torch.atleast_1d(time_pred.view(-1)).cpu().numpy()\n",
    "            effort_true_1d = torch.atleast_1d(effort).cpu().numpy()\n",
    "            time_true_1d = torch.atleast_1d(time_est).cpu().numpy()\n",
    "            \n",
    "            all_effort_preds.extend(effort_pred_1d.tolist())\n",
    "            all_effort_true.extend(effort_true_1d.tolist())\n",
    "            all_time_preds.extend(time_pred_1d.tolist())\n",
    "            all_time_true.extend(time_true_1d.tolist())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_effort_loss = effort_loss / len(loader)\n",
    "    avg_time_loss = time_loss / len(loader)\n",
    "    mae_effort = mean_absolute_error(np.expm1(np.array(all_effort_true)), np.expm1(np.array(all_effort_preds)))\n",
    "    mae_time = mean_absolute_error(np.expm1(np.array(all_time_true)), np.expm1(np.array(all_time_preds)))\n",
    "    \n",
    "    return avg_loss, mae_effort, mae_time, avg_effort_loss, avg_time_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 14:20:33,808 - INFO - Usando dispositivo: cpu\n",
      "2025-10-13 14:20:33,818 - INFO - Dataset filtrado con 324 registros.\n",
      "2025-10-13 14:20:33,819 - INFO - Aplicando ingeniería de características robusta...\n",
      "2025-10-13 14:20:33,940 - INFO - Ingeniería de características robusta finalizada con éxito.\n",
      "2025-10-13 14:20:33,942 - INFO - Distribución 'effort' (original): count    324.000000\n",
      "mean       8.015432\n",
      "std        3.288465\n",
      "min        2.000000\n",
      "25%        5.000000\n",
      "50%        8.000000\n",
      "75%       13.000000\n",
      "max       13.000000\n",
      "Name: effort, dtype: float64\n",
      "2025-10-13 14:20:33,944 - INFO - Distribución 'time' (original): count    324.000000\n",
      "mean      12.317901\n",
      "std        5.412477\n",
      "min        3.000000\n",
      "25%        8.000000\n",
      "50%       12.000000\n",
      "75%       18.000000\n",
      "max       25.000000\n",
      "Name: time, dtype: float64\n",
      "2025-10-13 14:20:36,996 - INFO - Vocabulario construido con 2300 tokens.\n",
      "2025-10-13 14:20:37,066 - INFO - Iniciando entrenamiento...\n",
      "2025-10-13 14:21:52,903 - INFO - Nuevo mejor modelo guardado con Val Loss: 11.6769\n",
      "2025-10-13 14:23:10,103 - INFO - Nuevo mejor modelo guardado con Val Loss: 0.8630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[277]\u001b[39m\u001b[32m, line 184\u001b[39m\n\u001b[32m    181\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScaler guardado en: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.SCALER_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Para ejecutar el pipeline:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[277]\u001b[39m\u001b[32m, line 169\u001b[39m, in \u001b[36mrun_training_pipeline\u001b[39m\u001b[34m(use_pretrained_embeddings)\u001b[39m\n\u001b[32m    157\u001b[39m model = EstimationLSTM(\n\u001b[32m    158\u001b[39m     vocab_size=\u001b[38;5;28mlen\u001b[39m(text_processor.vocab),\n\u001b[32m    159\u001b[39m     embedding_dim=config.EMBEDDING_DIM,\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m     pretrained_embeddings=pretrained_embeddings\n\u001b[32m    166\u001b[39m ).to(device)\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# --- 7. Entrenamiento ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m history = \u001b[43m_perform_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# --- 8. Visualización de Resultados ---\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m history:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[277]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36m_perform_training_loop\u001b[39m\u001b[34m(model, train_loader, val_loader, config, device)\u001b[39m\n\u001b[32m     32\u001b[39m seq, numerical, effort, time_est = (t.to(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m [seq, numerical, effort, time_est])\n\u001b[32m     34\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m effort_pred, time_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Calcular pérdida ponderada\u001b[39;00m\n\u001b[32m     38\u001b[39m loss_effort = criterion_regression(effort_pred.squeeze(), effort)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[270]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mEstimationLSTM.forward\u001b[39m\u001b[34m(self, text, numerical_features)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: torch.Tensor, numerical_features: torch.Tensor):\n\u001b[32m     26\u001b[39m     embedded = \u001b[38;5;28mself\u001b[39m.embedding(text)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     last_hidden = out[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m     29\u001b[39m     last_hidden = \u001b[38;5;28mself\u001b[39m.dropout(last_hidden)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def _load_and_prepare_data(config):\n",
    "    \"\"\"Carga el dataset desde un CSV, aplica transformaciones y filtros.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(config.CSV_PATH)\n",
    "        # Transformación logarítmica para reducir el sesgo de valores extremos\n",
    "        df['effort'] = np.log1p(df['effort'])\n",
    "        df['time'] = np.log1p(df['time'])\n",
    "        # Filtrado para eliminar outliers en la escala logarítmica\n",
    "        df = df[df['time'] <= np.log1p(25)]\n",
    "        logging.info(f\"Dataset filtrado con {len(df)} registros.\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Archivo no encontrado en {config.CSV_PATH}. Abortando.\")\n",
    "        return None\n",
    "\n",
    "def _perform_training_loop(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"Ejecuta el bucle de entrenamiento y validación completo.\"\"\"\n",
    "    criterion_regression = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "    history = {'train_losses': [], 'val_losses': [], 'mae_efforts': [], 'mae_times': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    logging.info(\"Iniciando entrenamiento...\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for seq, numerical, effort, time_est in train_loader:\n",
    "            # Mover datos al dispositivo\n",
    "            seq, numerical, effort, time_est = (t.to(device) for t in [seq, numerical, effort, time_est])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            effort_pred, time_pred = model(seq, numerical)\n",
    "            \n",
    "            # Calcular pérdida ponderada\n",
    "            loss_effort = criterion_regression(effort_pred.squeeze(), effort)\n",
    "            loss_time = criterion_regression(time_pred.squeeze(), time_est)\n",
    "            loss = (config.LOSS_WEIGHTS['effort'] * loss_effort) + (config.LOSS_WEIGHTS['time'] * loss_time)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        history['train_losses'].append(avg_train_loss)\n",
    "        \n",
    "        # Evaluación del modelo\n",
    "        val_loss, mae_effort, mae_time, avg_effort_loss, avg_time_loss = evaluate_model(\n",
    "            model, val_loader, device, criterion_regression, config.LOSS_WEIGHTS\n",
    "        )\n",
    "        history['val_losses'].append(val_loss)\n",
    "        history['mae_efforts'].append(mae_effort)\n",
    "        history['mae_times'].append(mae_time)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            logging.info(f\"Epoch [{epoch+1}/{config.NUM_EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            logging.info(f\"MAE Effort: {mae_effort:.4f} horas | MAE Time: {mae_time:.4f} horas\")\n",
    "\n",
    "        # Lógica de Early Stopping y guardado del mejor modelo\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), config.MODEL_PATH)\n",
    "            logging.info(f\"Nuevo mejor modelo guardado con Val Loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config.PATIENCE:\n",
    "                logging.info(f\"Early stopping activado en la época {epoch+1}.\")\n",
    "                break\n",
    "    \n",
    "    return history\n",
    "\n",
    "def _generate_charts(history):\n",
    "    \"\"\"Genera y muestra las configuraciones JSON para las gráficas de Chart.js.\"\"\"\n",
    "    epochs = list(range(1, len(history['train_losses']) + 1))\n",
    "    \n",
    "    chart_loss_config = {\n",
    "        \"type\": \"line\",\n",
    "        \"data\": {\n",
    "            \"labels\": epochs,\n",
    "            \"datasets\": [\n",
    "                {\"label\": \"Train Loss\", \"data\": history['train_losses'], \"borderColor\": \"#1f77b4\", \"fill\": False},\n",
    "                {\"label\": \"Validation Loss\", \"data\": history['val_losses'], \"borderColor\": \"#ff7f0e\", \"fill\": False}\n",
    "            ]\n",
    "        },\n",
    "        \"options\": {\"title\": {\"display\": True, \"text\": \"Curvas de Pérdida (Training y Validation)\"}}\n",
    "    }\n",
    "    print(\"```chartjs\\n\" + json.dumps(chart_loss_config, indent=2) + \"\\n```\")\n",
    "    \n",
    "    chart_mae_config = {\n",
    "        \"type\": \"line\",\n",
    "        \"data\": {\n",
    "            \"labels\": epochs,\n",
    "            \"datasets\": [\n",
    "                {\"label\": \"MAE Effort (horas)\", \"data\": history['mae_efforts'], \"borderColor\": \"#2ca02c\", \"fill\": False},\n",
    "                {\"label\": \"MAE Time (horas)\", \"data\": history['mae_times'], \"borderColor\": \"#d62728\", \"fill\": False}\n",
    "            ]\n",
    "        },\n",
    "        \"options\": {\"title\": {\"display\": True, \"text\": \"Error Absoluto Medio (MAE)\"}}\n",
    "    }\n",
    "    print(\"```chartjs\\n\" + json.dumps(chart_mae_config, indent=2) + \"\\n```\")\n",
    "\n",
    "def run_training_pipeline(use_pretrained_embeddings=False):\n",
    "    \"\"\"Orquesta el pipeline completo de entrenamiento del modelo.\"\"\"\n",
    "    # --- 1. Configuración Inicial ---\n",
    "    config = TrainingConfig()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f\"Usando dispositivo: {device}\")\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # --- 2. Carga y Preparación de Datos ---\n",
    "    df = _load_and_prepare_data(config)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # --- 3. Extracción de Features y Preprocesamiento ---\n",
    "    df_featured = extract_features(df)\n",
    "    logging.info(f\"Distribución 'effort' (original): {np.expm1(df['effort']).describe()}\")\n",
    "    logging.info(f\"Distribución 'time' (original): {np.expm1(df['time']).describe()}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_featured[config.NUMERICAL_FEATURES])\n",
    "    \n",
    "    text_processor = TextProcessor(max_seq_len=config.MAX_SEQ_LEN, min_freq=config.MIN_WORD_FREQ, use_spacy=config.USE_SPACY)\n",
    "    text_processor.build_vocab(df_featured['full_text'])\n",
    "    \n",
    "    # --- 4. Carga de Embeddings Pre-entrenados (Opcional) ---\n",
    "    pretrained_embeddings = None\n",
    "    if use_pretrained_embeddings:\n",
    "        try:\n",
    "            import fasttext\n",
    "            model_ft = fasttext.load_model('cc.es.300.bin')\n",
    "            embedding_matrix = np.zeros((len(text_processor.vocab), config.EMBEDDING_DIM))\n",
    "            for word, idx in text_processor.vocab.items():\n",
    "                if word not in ['<PAD>', '<UNK>']:\n",
    "                    embedding_matrix[idx] = model_ft.get_word_vector(word)[:config.EMBEDDING_DIM]\n",
    "            pretrained_embeddings = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "            logging.info(\"Embeddings de FastText cargados correctamente.\")\n",
    "        except (ImportError, FileNotFoundError):\n",
    "            logging.warning(\"Módulo 'fasttext' no encontrado o archivo .bin no hallado. Usando embeddings entrenables.\")\n",
    "\n",
    "    # --- 5. Creación de Datasets y DataLoaders ---\n",
    "    dataset = StoryDataset(df_featured, text_processor, config.NUMERICAL_FEATURES, scaler)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # --- 6. Inicialización del Modelo ---\n",
    "    model = EstimationLSTM(\n",
    "        vocab_size=len(text_processor.vocab),\n",
    "        embedding_dim=config.EMBEDDING_DIM,\n",
    "        hidden_dim=config.HIDDEN_DIM,\n",
    "        num_features=len(config.NUMERICAL_FEATURES),\n",
    "        bidirectional=config.BIDIRECTIONAL,\n",
    "        dropout=config.DROPOUT,\n",
    "        num_layers=config.NUM_LAYERS,\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "\n",
    "    # --- 7. Entrenamiento ---\n",
    "    history = _perform_training_loop(model, train_loader, val_loader, config, device)\n",
    "\n",
    "    # --- 8. Visualización de Resultados ---\n",
    "    if history:\n",
    "        _generate_charts(history)\n",
    "\n",
    "    # --- 9. Guardado de Artefactos Finales ---\n",
    "    joblib.dump(scaler, config.SCALER_PATH)\n",
    "    text_processor.save(config.PROCESSOR_PATH)\n",
    "    logging.info(\"\\n¡Entrenamiento completado!\")\n",
    "    logging.info(f\"Modelo guardado en: {config.MODEL_PATH}\")\n",
    "    logging.info(f\"Procesador de texto guardado en: {config.PROCESSOR_PATH}\")\n",
    "    logging.info(f\"Scaler guardado en: {config.SCALER_PATH}\")\n",
    "\n",
    "# Para ejecutar el pipeline:\n",
    "run_training_pipeline(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones y Mejoras Sugeridas\n",
    "\n",
    "- **Fortalezas:** Código limpio, modular, con logging. Buen uso de PyTorch para multi-tarea.\n",
    "- **Problemas potenciales:** No maneja datasets grandes (vocab simple), no validación, umbrales hardcoded en complejidad.\n",
    "- **Mejoras en ML:**\n",
    "  - Usar cross-validation o train/test split.\n",
    "  - Embeddings pre-entrenados (e.g., via torchtext).\n",
    "  - Métricas adicionales: R2 para regresión, F1 para clasificación.\n",
    "  - Hiperparámetro tuning con Optuna.\n",
    "  - Manejo de imbalance si complejidad está desbalanceada.\n",
    "- **Próximos pasos:** Cargar un dataset real y entrenar. Visualizar métricas avanzadas (MAE, accuracy).\n",
    "\n",
    "Puedes ejecutar este notebook en Jupyter ajustando paths/data. Si necesitas más detalles, ¡pregunta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def descargar_y_descomprimir_fasttext():\n",
    "    \"\"\"\n",
    "    Descarga y descomprime el modelo pre-entrenado de FastText para español.\n",
    "    \"\"\"\n",
    "    # URL directa al modelo de vectores de Common Crawl (cc) para español (es)\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz\"\n",
    "    \n",
    "    # Nombres de los archivos\n",
    "    archivo_comprimido = \"cc.es.300.bin.gz\"\n",
    "    archivo_descomprimido = \"cc.es.300.bin\"\n",
    "\n",
    "    print(f\"Modelo a descargar: {archivo_comprimido}\")\n",
    "    \n",
    "    # --- Paso 1: Descargar el archivo con barra de progreso ---\n",
    "    try:\n",
    "        # Usamos stream=True para no cargar todo el archivo en memoria\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()  # Lanza un error si la descarga falla\n",
    "            \n",
    "            # Obtener el tamaño total del archivo desde las cabeceras\n",
    "            total_size_in_bytes = int(r.headers.get('content-length', 0))\n",
    "            \n",
    "            # Configurar la barra de progreso de tqdm\n",
    "            progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, desc=\"📥 Descargando modelo\")\n",
    "            \n",
    "            with open(archivo_comprimido, 'wb') as f:\n",
    "                # Descargar en bloques de 1MB\n",
    "                for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                    progress_bar.update(len(chunk))\n",
    "                    f.write(chunk)\n",
    "            progress_bar.close()\n",
    "\n",
    "        print(\"\\n✅ Descarga completada.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error durante la descarga: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Paso 2: Descomprimir el archivo .gz ---\n",
    "    print(f\"\\n⚙️ Descomprimiendo {archivo_comprimido}...\")\n",
    "    try:\n",
    "        with gzip.open(archivo_comprimido, 'rb') as f_in:\n",
    "            with open(archivo_descomprimido, 'wb') as f_out:\n",
    "                # Copiar el contenido descomprimido\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(\"✅ Descompresión finalizada.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error durante la descompresión: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Paso 3: Limpiar el archivo comprimido ---\n",
    "    try:\n",
    "        print(f\"\\n🗑️ Eliminando el archivo temporal {archivo_comprimido}...\")\n",
    "        os.remove(archivo_comprimido)\n",
    "        print(\"✅ Limpieza completada.\")\n",
    "        print(f\"\\n✨ ¡Listo! El modelo '{archivo_descomprimido}' está en tu carpeta.\")\n",
    "        \n",
    "    except OSError as e:\n",
    "        print(f\"❌ Error al eliminar el archivo temporal: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "descargar_y_descomprimir_fasttext()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
